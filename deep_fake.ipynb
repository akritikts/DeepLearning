{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CsAPbSnqyq7_"
   },
   "source": [
    "GitHub [link](https://github.com/MYUSER/MYPROJECT/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pxELm4Tfyq8A"
   },
   "source": [
    "\n",
    "Welcome to your assignment this week! \n",
    "\n",
    "To better understand the adverse use of AI, in this assignment, we will look at a Natural Language Processing use case.\n",
    "\n",
    "\n",
    "Natural Language Pocessing (NLP) is a branch of Artificial Intelligence (AI) that helps computers to understand, to interpret and to manipulate natural (i.e. human) language.\n",
    "Imagine NLP-powered machines as black boxes that are capable of understanding and evaluating the context of the input documents (i.e. collection of words), outputting meaningful results that depend on the task the machine is designed for.\n",
    "\n",
    "\n",
    "![](imgs/1_3zMvUnPzYZF9CSHdj6hT5A.png)\n",
    "\n",
    "<caption><center> Documents are fed into magic NLP model capable to get, for instance, the sentiment of the original content</center></caption>\n",
    "\n",
    "\n",
    "In this notebook, you will implement a model that uses an LSTM to generate fake tweets and comments. You will also be able to try it to generate your own fake text. \n",
    "\n",
    "**You will learn to:**\n",
    "- Apply an LSTM to generate fake comments.\n",
    "- Generate your own fake text with deep learning.\n",
    "\n",
    "Please run the following cell to load all the packages required in this assignment. This may take a few minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785
    },
    "colab_type": "code",
    "id": "Bmf-NjEpyq8B",
    "outputId": "88a4f267-efba-44c7-b514-8befde190d26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.4.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.31.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.12.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.17.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.23.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow) (49.2.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "emGW1aoFyq8F"
   },
   "source": [
    "Run the following cell to load the packages you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET8e2YZeyq8G"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QYCk7y0Iyq8J"
   },
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uXw87ucnyq8K"
   },
   "source": [
    "Let's define a tokenizer and read the data from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTjndl4ryq8L"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='\"#$%&()*+-/:;<=>@[\\\\]^_`{|}~\\t\\n')\n",
    "data = open('covid19_fake.txt').read().replace(\".\", \" . \").replace(\",\", \" , \").replace(\"?\", \" ? \").replace(\"!\", \" ! \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "LD_HYKb9y-tz",
    "outputId": "ab46cdc2-b3cc-4728-ac49-6c30f4586c82"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-8714512b-5aa4-4dd3-b18d-7c0b291c8f27\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-8714512b-5aa4-4dd3-b18d-7c0b291c8f27\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving covid19_fake.txt to covid19_fake.txt\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D_a2UCLxyq8O"
   },
   "source": [
    "Now, let's splits the data into tweets  where each line of the input file is a fake tweets.\n",
    "\n",
    "We also extract the vocabulary of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oeTbS3yFyq8O"
   },
   "outputs": [],
   "source": [
    "corpus = data.lower().split(\"\\n\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T-gJy1e8yq8S"
   },
   "source": [
    "You've loaded:\n",
    "- `corpus`: an array where each entry is a fake post.\n",
    "- `tokenizer`: which is the object that we will use to vectorize our dataset. This object also contains our word index.\n",
    "- `total_words`: is the total number of words in the vacabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "LRUq6_70yq8S",
    "outputId": "235bba5e-e220-4794-8f8c-de85b0562932"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of fake tweets:  ['there is already a vaccine to treat covid19 . ', 'cleaning hands do not help to prevent covid19 . ']\n",
      "Size of the vocabulary =  1257\n",
      "Example of our word index =  [('.', 1), ('the', 2), ('covid19', 3), ('in', 4), ('to', 5), ('a', 6), ('of', 7), (',', 8), ('coronavirus', 9), ('and', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of fake tweets: \",corpus[:2])\n",
    "print(\"Size of the vocabulary = \", total_words)\n",
    "index = [(k, v) for k, v in tokenizer.word_index.items()]\n",
    "print(\"Example of our word index = \", index[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QK1GfUIbyq8V"
   },
   "source": [
    "The next step aims to generate the training set of n_grams sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3xIrorgyq8W"
   },
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubIZ1Hinyq8Z"
   },
   "source": [
    "You've create:\n",
    "- `input_sequences`: which is a list of n_grams sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "t0WdXTE1yq8a",
    "outputId": "70feac85-b1e6-47d5-9014-4f74b2645074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[2, 3, 12, 187, 34, 188]\n",
      " and it corresponds to:\n",
      "the covid19 is same as sars "
     ]
    }
   ],
   "source": [
    "sample = 20\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\" and it corresponds to:\")\n",
    "for i in input_sequences[sample]:\n",
    "    print(reverse_word_map[i], end=' ')\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vdszEa-oyq8d"
   },
   "source": [
    "Next, we padd our training set to the max length in order to be able to make a batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JoQc4HIeyq8d"
   },
   "outputs": [],
   "source": [
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwxSGFzWyq8g"
   },
   "source": [
    "Run the following to see the containt of the padded 'input_sequences' object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "XUhPlqgVyq8g",
    "outputId": "23856570-3263-4e8e-aed5-724e7fca85c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34 188]\n",
      " and it corresponds to:\n",
      "[ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ __ the covid19 is same as sars ]\n"
     ]
    }
   ],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\" and it corresponds to:\")\n",
    "print(\"[\", end=' ')\n",
    "for i in input_sequences[sample]:\n",
    "    if i in reverse_word_map:\n",
    "        print(reverse_word_map[i], end=' ')\n",
    "    else:\n",
    "        print(\"__\", end=' ')\n",
    "print(\"]\")\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLSkSBwNyq8j"
   },
   "source": [
    "Given a sentence like **\"the covid19 is same as \"**, we want to design a model that can predict the next word -- in the case the word **\"sars\"**.\n",
    "\n",
    "Therefore, the next code prepares our input and output to our model consequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pZVZkaqyq8k"
   },
   "outputs": [],
   "source": [
    "input_to_model, label = input_sequences[:,:-1],input_sequences[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "EiShHpAbyq8n",
    "outputId": "df61353a-f461-497d-ec12-c65bf9b5a4a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The entry  20  in 'input_sequences' is: \n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34 188]\n",
      ", it corresponds to the following input to our model:\n",
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   2   3  12 187  34]\n",
      " and the following output:  188\n"
     ]
    }
   ],
   "source": [
    "print(\"The entry \",sample,\" in 'input_sequences' is: \")\n",
    "print(input_sequences[sample])\n",
    "print(\", it corresponds to the following input to our model:\")\n",
    "print(input_to_model[sample])\n",
    "print(\" and the following output: \", label[sample])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pfPaqNGyyq8p"
   },
   "source": [
    "Finally, we convert our label to categorical labels for being processed by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H403og1Jyq8q"
   },
   "outputs": [],
   "source": [
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JYZttjgqyq8s"
   },
   "source": [
    "Here is the architecture of the model we will use:\n",
    "\n",
    "![](imgs/text_generation.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8a2OVsEEyq8t"
   },
   "source": [
    " \n",
    "**Task 1**: Implement `deep_fake_comment_model()`. You will need to carry out 5 steps:\n",
    "\n",
    "1. Create a sequencial model using the `Sequential` class\n",
    "2. Add an embedding layer to the model using the `Embedding` class of size 128\n",
    "3. Add an LSTM layer to the model using the `LSTM` class of size 128\n",
    "4. Add a Dense layer to the model using the `Dense` class with a `softmax` activation\n",
    "5. Set a `categorical_crossentropy` loss function to the model and optimize `accuracy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWswLempyq8u"
   },
   "outputs": [],
   "source": [
    "#TASK 1\n",
    "# deep_fake_comment_model\n",
    "\n",
    "def deep_fake_comment_model():\n",
    "    ### START CODE HERE ### \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=1258,output_dim=128,input_length=60))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1257, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "    \n",
    "#Print details of the model.\n",
    "model = deep_fake_comment_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-CfpaIJKyq8w"
   },
   "source": [
    "Now, let's start our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6817
    },
    "colab_type": "code",
    "id": "3XHqfu1zyq8x",
    "outputId": "95618f1d-7286-42f2-84b8-7f2d3588e0f2",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "85/85 [==============================] - 1s 15ms/step - loss: 0.1207 - val_loss: 0.1368\n",
      "Epoch 2/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1161 - val_loss: 0.1503\n",
      "Epoch 3/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1142 - val_loss: 0.1597\n",
      "Epoch 4/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1134 - val_loss: 0.1683\n",
      "Epoch 5/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1125 - val_loss: 0.1759\n",
      "Epoch 6/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1114 - val_loss: 0.1835\n",
      "Epoch 7/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1107 - val_loss: 0.1874\n",
      "Epoch 8/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1108 - val_loss: 0.1937\n",
      "Epoch 9/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1104 - val_loss: 0.1988\n",
      "Epoch 10/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1104 - val_loss: 0.2009\n",
      "Epoch 11/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1099 - val_loss: 0.2064\n",
      "Epoch 12/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1096 - val_loss: 0.2109\n",
      "Epoch 13/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1098 - val_loss: 0.2150\n",
      "Epoch 14/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1102 - val_loss: 0.2141\n",
      "Epoch 15/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1087 - val_loss: 0.2168\n",
      "Epoch 16/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1091 - val_loss: 0.2204\n",
      "Epoch 17/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1091 - val_loss: 0.2222\n",
      "Epoch 18/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1090 - val_loss: 0.2250\n",
      "Epoch 19/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1092 - val_loss: 0.2269\n",
      "Epoch 20/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1082 - val_loss: 0.2305\n",
      "Epoch 21/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1083 - val_loss: 0.2313\n",
      "Epoch 22/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1087 - val_loss: 0.2363\n",
      "Epoch 23/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1092 - val_loss: 0.2369\n",
      "Epoch 24/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1087 - val_loss: 0.2419\n",
      "Epoch 25/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1099 - val_loss: 0.2426\n",
      "Epoch 26/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1086 - val_loss: 0.2436\n",
      "Epoch 27/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1090 - val_loss: 0.2472\n",
      "Epoch 28/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1083 - val_loss: 0.2483\n",
      "Epoch 29/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1082 - val_loss: 0.2481\n",
      "Epoch 30/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1087 - val_loss: 0.2528\n",
      "Epoch 31/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1084 - val_loss: 0.2554\n",
      "Epoch 32/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1091 - val_loss: 0.2560\n",
      "Epoch 33/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1082 - val_loss: 0.2567\n",
      "Epoch 34/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1079 - val_loss: 0.2583\n",
      "Epoch 35/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1076 - val_loss: 0.2621\n",
      "Epoch 36/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1078 - val_loss: 0.2628\n",
      "Epoch 37/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1078 - val_loss: 0.2653\n",
      "Epoch 38/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1081 - val_loss: 0.2660\n",
      "Epoch 39/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1075 - val_loss: 0.2673\n",
      "Epoch 40/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1080 - val_loss: 0.2718\n",
      "Epoch 41/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1077 - val_loss: 0.2722\n",
      "Epoch 42/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1080 - val_loss: 0.2716\n",
      "Epoch 43/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1073 - val_loss: 0.2753\n",
      "Epoch 44/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1078 - val_loss: 0.2773\n",
      "Epoch 45/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1072 - val_loss: 0.2786\n",
      "Epoch 46/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1079 - val_loss: 0.2829\n",
      "Epoch 47/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1085 - val_loss: 0.2837\n",
      "Epoch 48/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.2865\n",
      "Epoch 49/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.2866\n",
      "Epoch 50/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.2870\n",
      "Epoch 51/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1078 - val_loss: 0.2904\n",
      "Epoch 52/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.2922\n",
      "Epoch 53/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.2947\n",
      "Epoch 54/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1080 - val_loss: 0.2974\n",
      "Epoch 55/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1080 - val_loss: 0.2960\n",
      "Epoch 56/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.3019\n",
      "Epoch 57/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1075 - val_loss: 0.3024\n",
      "Epoch 58/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1078 - val_loss: 0.3032\n",
      "Epoch 59/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1079 - val_loss: 0.3066\n",
      "Epoch 60/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1075 - val_loss: 0.3072\n",
      "Epoch 61/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.3106\n",
      "Epoch 62/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1076 - val_loss: 0.3098\n",
      "Epoch 63/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.3149\n",
      "Epoch 64/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.3151\n",
      "Epoch 65/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1079 - val_loss: 0.3181\n",
      "Epoch 66/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1078 - val_loss: 0.3339\n",
      "Epoch 67/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1658 - val_loss: 0.5227\n",
      "Epoch 68/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1362 - val_loss: 0.4789\n",
      "Epoch 69/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1119 - val_loss: 0.4608\n",
      "Epoch 70/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1089 - val_loss: 0.4601\n",
      "Epoch 71/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1082 - val_loss: 0.4606\n",
      "Epoch 72/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1082 - val_loss: 0.4627\n",
      "Epoch 73/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1079 - val_loss: 0.4611\n",
      "Epoch 74/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1078 - val_loss: 0.4617\n",
      "Epoch 75/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1080 - val_loss: 0.4645\n",
      "Epoch 76/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1084 - val_loss: 0.4667\n",
      "Epoch 77/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1081 - val_loss: 0.4675\n",
      "Epoch 78/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.4694\n",
      "Epoch 79/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1076 - val_loss: 0.4705\n",
      "Epoch 80/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.4753\n",
      "Epoch 81/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.4737\n",
      "Epoch 82/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1076 - val_loss: 0.4741\n",
      "Epoch 83/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1073 - val_loss: 0.4767\n",
      "Epoch 84/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.4796\n",
      "Epoch 85/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1071 - val_loss: 0.4812\n",
      "Epoch 86/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.4844\n",
      "Epoch 87/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.4858\n",
      "Epoch 88/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1071 - val_loss: 0.4850\n",
      "Epoch 89/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.4867\n",
      "Epoch 90/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1080 - val_loss: 0.4881\n",
      "Epoch 91/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.4874\n",
      "Epoch 92/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1073 - val_loss: 0.4905\n",
      "Epoch 93/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1073 - val_loss: 0.4897\n",
      "Epoch 94/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.4917\n",
      "Epoch 95/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1067 - val_loss: 0.4930\n",
      "Epoch 96/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1070 - val_loss: 0.4962\n",
      "Epoch 97/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1077 - val_loss: 0.4938\n",
      "Epoch 98/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1081 - val_loss: 0.4993\n",
      "Epoch 99/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1074 - val_loss: 0.4992\n",
      "Epoch 100/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1072 - val_loss: 0.5037\n",
      "Epoch 101/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.5036\n",
      "Epoch 102/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1075 - val_loss: 0.5044\n",
      "Epoch 103/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1074 - val_loss: 0.5005\n",
      "Epoch 104/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1084 - val_loss: 0.5027\n",
      "Epoch 105/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1075 - val_loss: 0.5054\n",
      "Epoch 106/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 0.5095\n",
      "Epoch 107/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.5105\n",
      "Epoch 108/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1075 - val_loss: 0.5139\n",
      "Epoch 109/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1064 - val_loss: 0.5166\n",
      "Epoch 110/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1068 - val_loss: 0.5176\n",
      "Epoch 111/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.5199\n",
      "Epoch 112/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.5239\n",
      "Epoch 113/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.5217\n",
      "Epoch 114/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.5256\n",
      "Epoch 115/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1076 - val_loss: 0.5284\n",
      "Epoch 116/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1073 - val_loss: 0.5306\n",
      "Epoch 117/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.5315\n",
      "Epoch 118/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.5391\n",
      "Epoch 119/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.5401\n",
      "Epoch 120/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1078 - val_loss: 0.5460\n",
      "Epoch 121/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1069 - val_loss: 0.5447\n",
      "Epoch 122/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.5460\n",
      "Epoch 123/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.5478\n",
      "Epoch 124/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1071 - val_loss: 0.5530\n",
      "Epoch 125/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.5551\n",
      "Epoch 126/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1068 - val_loss: 0.5562\n",
      "Epoch 127/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.5579\n",
      "Epoch 128/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.5622\n",
      "Epoch 129/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.5677\n",
      "Epoch 130/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.5651\n",
      "Epoch 131/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1067 - val_loss: 0.5680\n",
      "Epoch 132/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.5750\n",
      "Epoch 133/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.5777\n",
      "Epoch 134/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.5795\n",
      "Epoch 135/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1074 - val_loss: 0.5840\n",
      "Epoch 136/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1065 - val_loss: 0.5842\n",
      "Epoch 137/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.5848\n",
      "Epoch 138/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1069 - val_loss: 0.5905\n",
      "Epoch 139/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.6021\n",
      "Epoch 140/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1071 - val_loss: 0.5993\n",
      "Epoch 141/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1065 - val_loss: 0.5991\n",
      "Epoch 142/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.5985\n",
      "Epoch 143/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.6050\n",
      "Epoch 144/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 0.6072\n",
      "Epoch 145/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.6104\n",
      "Epoch 146/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1077 - val_loss: 0.6182\n",
      "Epoch 147/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1071 - val_loss: 0.6203\n",
      "Epoch 148/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 0.6243\n",
      "Epoch 149/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.6254\n",
      "Epoch 150/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 0.6356\n",
      "Epoch 151/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.6424\n",
      "Epoch 152/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.6363\n",
      "Epoch 153/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.6428\n",
      "Epoch 154/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.6422\n",
      "Epoch 155/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.6533\n",
      "Epoch 156/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.6529\n",
      "Epoch 157/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1073 - val_loss: 0.6630\n",
      "Epoch 158/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.6673\n",
      "Epoch 159/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.6649\n",
      "Epoch 160/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1068 - val_loss: 0.6702\n",
      "Epoch 161/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1062 - val_loss: 0.6811\n",
      "Epoch 162/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1073 - val_loss: 0.6820\n",
      "Epoch 163/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1072 - val_loss: 0.6790\n",
      "Epoch 164/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1073 - val_loss: 0.6878\n",
      "Epoch 165/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1069 - val_loss: 0.6941\n",
      "Epoch 166/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1069 - val_loss: 0.7006\n",
      "Epoch 167/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.7021\n",
      "Epoch 168/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1072 - val_loss: 0.7074\n",
      "Epoch 169/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1074 - val_loss: 0.7071\n",
      "Epoch 170/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1062 - val_loss: 0.7205\n",
      "Epoch 171/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.7264\n",
      "Epoch 172/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1066 - val_loss: 0.7258\n",
      "Epoch 173/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1075 - val_loss: 0.7380\n",
      "Epoch 174/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.7248\n",
      "Epoch 175/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1067 - val_loss: 0.7440\n",
      "Epoch 176/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1069 - val_loss: 0.7418\n",
      "Epoch 177/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.7503\n",
      "Epoch 178/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1068 - val_loss: 0.7580\n",
      "Epoch 179/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1070 - val_loss: 0.7564\n",
      "Epoch 180/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1065 - val_loss: 0.7625\n",
      "Epoch 181/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.7791\n",
      "Epoch 182/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1064 - val_loss: 0.7714\n",
      "Epoch 183/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1062 - val_loss: 0.7823\n",
      "Epoch 184/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1064 - val_loss: 0.7863\n",
      "Epoch 185/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.7813\n",
      "Epoch 186/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1064 - val_loss: 0.7879\n",
      "Epoch 187/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1063 - val_loss: 0.7960\n",
      "Epoch 188/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.8051\n",
      "Epoch 189/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1061 - val_loss: 0.8118\n",
      "Epoch 190/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1067 - val_loss: 0.8229\n",
      "Epoch 191/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.8232\n",
      "Epoch 192/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1065 - val_loss: 0.8232\n",
      "Epoch 193/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1070 - val_loss: 0.8332\n",
      "Epoch 194/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.8435\n",
      "Epoch 195/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1066 - val_loss: 0.8425\n",
      "Epoch 196/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1067 - val_loss: 0.8675\n",
      "Epoch 197/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1071 - val_loss: 0.8575\n",
      "Epoch 198/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1067 - val_loss: 0.8618\n",
      "Epoch 199/200\n",
      "85/85 [==============================] - 1s 10ms/step - loss: 0.1070 - val_loss: 0.8563\n",
      "Epoch 200/200\n",
      "85/85 [==============================] - 1s 9ms/step - loss: 0.1068 - val_loss: 0.8742\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_to_model, label, epochs=200, batch_size=32, verbose=1, validation_split=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tIRAi08Myq81"
   },
   "source": [
    "Let's plot details of our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "colab_type": "code",
    "id": "DxJaIk7Xyq81",
    "outputId": "33dcd016-74e5-4130-bea4-5e85a9ba1d70"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de3xU9ZnH8c9DQkDBC0KsSFCgRRSBRQ0ouCq2uxbUgq5SYVkUexFtvZRqhdbWWmtbvHS1trpeetFVK+Iqikq9VsV6JVBQEVDEKAFUREVRbkme/eN3hhliQiaQM3My+b5fr7zmzLk+c2byPb/zO2cSc3dERKRwtcl3ASIiEi8FvYhIgVPQi4gUOAW9iEiBU9CLiBQ4Bb2ISIFT0EvBM7O/mdlpzT1vE2sYZmZVzb1ekWwU57sAkfqY2bqMpzsDG4Ga6PlEd78j23W5+4g45hVpKRT0kkju3jE1bGaVwHfc/fG685lZsbtX57I2kZZGXTfSoqS6QMxsspm9C/zFzDqZ2YNmttrMPoqGyzKWecrMvhMNTzCzf5jZVdG8b5nZiO2ct6eZzTazT83scTO7zsxuz/J1HBBt62MzW2hmIzOmHWtmr0XrXWFmF0Tju0Sv7WMz+9DMnjEz/Q5Lo/QhkZZoL2APYF/gDMLn+C/R832A9cAftrH8ocASoAtwBfAnM7PtmPevwEtAZ+ASYHw2xZtZW+AB4FFgT+Ac4A4z6xPN8idC99QuQD/g79H484EqoBT4EvATQH/DRBqloJeWqBb4ubtvdPf17r7G3e9x98/d/VPgV8BR21j+bXe/2d1rgFuBroTgzHpeM9sHGARc7O6b3P0fwMws6z8M6AhMjZb9O/AgMDaavhnoa2a7uvtH7j4vY3xXYF933+zuz7j+WJVkQUEvLdFqd9+QemJmO5vZjWb2tpl9AswGdjezogaWfzc14O6fR4Mdmzjv3sCHGeMAlmdZ/97AcnevzRj3NtAtGj4JOBZ428yeNrMh0fgrgaXAo2a2zMymZLk9aeUU9NIS1W3Fng/0AQ51912BI6PxDXXHNIdVwB5mtnPGuO5ZLrsS6F6nf30fYAWAu89x91GEbp37gOnR+E/d/Xx37wWMBH5oZl/bwdchrYCCXgrBLoR++Y/NbA/g53Fv0N3fBiqAS8ysJGp1fyPLxV8EPgcuNLO2ZjYsWnZatK5xZrabu28GPiF0VWFmx5vZV6JrBGsJt5vW1r8JkTQFvRSCa4CdgA+AF4CHc7TdccAQYA1wGXAX4X7/bXL3TYRgH0Go+XrgVHdfHM0yHqiMuqHOjLYD0Bt4HFgHPA9c7+5PNturkYJlupYj0jzM7C5gsbvHfkYh0hRq0YtsJzMbZGZfNrM2ZjYcGEXoUxdJFH0zVmT77QXcS7iPvgo4y93/md+SRL5IXTciIgVOXTciIgUucV03Xbp08R49euS7DBGRFmXu3LkfuHtpfdMSF/Q9evSgoqIi32WIiLQoZvZ2Q9PUdSMiUuAU9CIiBU5BLyJS4BLXRy8iybN582aqqqrYsGFD4zNLrNq3b09ZWRlt27bNehkFvYg0qqqqil122YUePXrQ8P9okbi5O2vWrKGqqoqePXtmvZy6bkSkURs2bKBz584K+TwzMzp37tzkMysFvYhkRSGfDNvzPijoJVa33w7r1uW7CpHWTUEvsamshPHj4f77812JtHRr1qxh4MCBDBw4kL322otu3bpteb5p06ZtLltRUcG5557b6DaGDh3aLLU+9dRTHH/88c2yruaii7ESm9Tv38ZG/xWHyLZ17tyZ+fPnA3DJJZfQsWNHLrjggi3Tq6urKS6uP87Ky8spLy9vdBvPPfdc8xSbQGrRS2xqo39yV12d3zqkME2YMIEzzzyTQw89lAsvvJCXXnqJIUOGcNBBBzF06FCWLFkCbN3CvuSSS/jWt77FsGHD6NWrF9dee+2W9XXs2HHL/MOGDePkk09m//33Z9y4caT+yu+sWbPYf//9OeSQQzj33HOb1HK/88476d+/P/369WPy5MkA1NTUMGHCBPr160f//v25+uqrAbj22mvp27cvAwYMYMyYMTu8r9Sil9go6AvTD34AUeO62QwcCNdc0/TlqqqqeO655ygqKuKTTz7hmWeeobi4mMcff5yf/OQn3HPPPV9YZvHixTz55JN8+umn9OnTh7POOusL96T/85//ZOHChey9994cfvjhPPvss5SXlzNx4kRmz55Nz549GTt2bNZ1rly5ksmTJzN37lw6derEMcccw3333Uf37t1ZsWIFr776KgAff/wxAFOnTuWtt96iXbt2W8btCLXoJTYKeonb6NGjKSoqAmDt2rWMHj2afv36MWnSJBYuXFjvMscddxzt2rWjS5cu7Lnnnrz33ntfmGfw4MGUlZXRpk0bBg4cSGVlJYsXL6ZXr15b7l9vStDPmTOHYcOGUVpaSnFxMePGjWP27Nn06tWLZcuWcc455/Dwww+z6667AjBgwADGjRvH7bff3mCXVFOoRS+xUdAXpu1pecelQ4cOW4Z/9rOfcfTRRzNjxgwqKysZNmxYvcu0a9duy3BRURHV9XxAs5mnOXTq1IkFCxbwyCOPcMMNNzB9+nT+/Oc/89BDDzF79mweeOABfvWrX/HKK6/sUOCrRS+xUdBLLq1du5Zu3boBcMsttzT7+vv06cOyZcuorKwE4K677sp62cGDB/P000/zwQcfUFNTw5133slRRx3FBx98QG1tLSeddBKXXXYZ8+bNo7a2luXLl3P00Udz+eWXs3btWtbt4D3KatFLbBT0kksXXnghp512GpdddhnHHXdcs69/p5124vrrr2f48OF06NCBQYMGNTjvE088QVlZ2Zbnd999N1OnTuXoo4/G3TnuuOMYNWoUCxYs4PTTT6c2+mX5zW9+Q01NDf/1X//F2rVrcXfOPfdcdt999x2qPXH/M7a8vNz1j0cKQ0UFDBoEv/gFXHxxvquRHbFo0SIOOOCAfJeRd+vWraNjx464O9///vfp3bs3kyZNynkd9b0fZjbX3eu9j1RdNxIbteil0Nx8880MHDiQAw88kLVr1zJx4sR8l5QVdd1IbBT0UmgmTZqUlxb8jlKLXmJTUxMeFfSFIWndvK3V9rwPCnqJjVr0haN9+/asWbNGYZ9nqb9H3759+yYtp64biY2CvnCUlZVRVVXF6tWr811Kq5f6D1NNoaCX2CjoC0fbtm2b9B+NJFnUdSOxSQV9qq9eRPIjq6A3s+FmtsTMlprZlHqmH2lm88ys2sxOrjNtHzN71MwWmdlrZtajeUqXpFOLXiQZGg16MysCrgNGAH2BsWbWt85s7wATgL/Ws4r/Ba509wOAwcD7O1KwtBwKepFkyKaPfjCw1N2XAZjZNGAU8FpqBnevjKbVZi4YHRCK3f2xaD79U7lWREEvkgzZdN10A5ZnPK+KxmVjP+BjM7vXzP5pZldGZwhbMbMzzKzCzCp0Vb9wKOhFkiHui7HFwBHABcAgoBehi2cr7n6Tu5e7e3lpaWnMJUmuKOhFkiGboF8BdM94XhaNy0YVMN/dl7l7NXAfcHDTSpSWSkEvkgzZBP0coLeZ9TSzEmAMMDPL9c8BdjezVDP9q2T07UthU9CLJEOjQR+1xM8GHgEWAdPdfaGZXWpmIwHMbJCZVQGjgRvNbGG0bA2h2+YJM3sFMODmeF6KJI2CXiQZsvpmrLvPAmbVGXdxxvAcQpdOfcs+BgzYgRqlhVLQiySDvhkrsVHQiySDgl5io6AXSQYFvcRGQS+SDAp6iY2CXiQZFPQSGwW9SDIo6CU2CnqRZFDQS2wU9CLJoKCX2CjoRZJBQS+xUdCLJIOCXmKjoBdJBgW9xEZBL5IMCnqJTeqfgivoRfJLQS+xUYteJBkU9BIbBb1IMijoJTYKepFkUNBLbBT0IsmgoJfYpIK+tjY9LCK5p6CX2GSGe+oOHBHJPQW9xCYz6NV9I5I/CnqJjYJeJBkU9BIbBb1IMijoJTYKepFkUNBLbBT0IsmgoJfYKOhFkiGroDez4Wa2xMyWmtmUeqYfaWbzzKzazE6uM63GzOZHPzObq3BJPgW9SDIUNzaDmRUB1wH/DlQBc8xspru/ljHbO8AE4IJ6VrHe3Qc2Q63SwijoRZKh0aAHBgNL3X0ZgJlNA0YBW4Le3Sujafr+o2yhoBdJhmy6broByzOeV0XjstXezCrM7AUzO6FJ1UmLpqAXSYZsWvQ7al93X2FmvYC/m9kr7v5m5gxmdgZwBsA+++yTg5IkFxT0IsmQTYt+BdA943lZNC4r7r4ielwGPAUcVM88N7l7ubuXl5aWZrtqSTgFvUgyZBP0c4DeZtbTzEqAMUBWd8+YWSczaxcNdwEOJ6NvXwqbgl4kGRoNenevBs4GHgEWAdPdfaGZXWpmIwHMbJCZVQGjgRvNbGG0+AFAhZktAJ4Epta5W0cKmIJeJBmy6qN391nArDrjLs4YnkPo0qm73HNA/x2sUVooBb1IMuibsRIbBb1IMijoJTYKepFkUNBLbDL/q5SCXiR/FPQSG7XoRZJBQS+xUdCLJIOCXmKjoBdJBgW9xKa2FoqKwrCCXiR/FPQSm9paKCkJwwp6kfxR0EtsFPQiyaCgl9go6EWSQUEvsVHQiySDgl5io6AXSQYFvcRGQS+SDAp6iY2CXiQZFPQSGwW9SDIo6CU2tbVQHP3HAwW9SP4o6CU2qW/GFhcr6EXySUEvsamthTZtFPQi+aagl9go6EWSQUEvsVHQiySDgl5io6AXSQYFvcRGQS+SDAp6iY2CXiQZFPQSG91eKZIMCnqJjVr0IsmQVdCb2XAzW2JmS81sSj3TjzSzeWZWbWYn1zN9VzOrMrM/NEfR0jIo6EWSodGgN7Mi4DpgBNAXGGtmfevM9g4wAfhrA6v5JTB7+8uUlkhBL5IM2bToBwNL3X2Zu28CpgGjMmdw90p3fxmorbuwmR0CfAl4tBnqlRakpkZBL5IE2QR9N2B5xvOqaFyjzKwN8FvggkbmO8PMKsysYvXq1dmsWloAtehFkiHui7HfA2a5e9W2ZnL3m9y93N3LS0tLYy5JckVBL5IMxVnMswLonvG8LBqXjSHAEWb2PaAjUGJm69z9Cxd0pfAo6EWSIZugnwP0NrOehIAfA/xnNit393GpYTObAJQr5FsPBb1IMjTadePu1cDZwCPAImC6uy80s0vNbCSAmQ0ysypgNHCjmS2Ms2hpGRT0IsmQTYsed58FzKoz7uKM4TmELp1treMW4JYmVygtloJeJBn0zViJjYJeJBkU9BIbBb1IMijoJTYKepFkUNBLbBT0IsmgoJfYKOhFkkFBL7FR0Iskg4JeYpMZ9Js357sakdZLQS+xSQV927YKepF8UtBLbBT0IsmQ1TdjRbZHKujN1Ecvkk8KeomN+uhFkkFdNxIbdd2IJIOCXmKTGfTu4V8LikjuKeglNplBD2rVi+SLgl5iU1sLRUUKepF8U9BLbNSiF0kGBb3Ewj38KOhF8k9BL7GorQ2PqdsrQffSi+SLgl5ikRn0atGL5JeCXmKhoBdJDgW9xEJBL5IcCnqJhYJeJDkU9BILBb1IcijoJRYKepHkyCrozWy4mS0xs6VmNqWe6Uea2TwzqzazkzPG7xuNn29mC83szOYsXpKrvtsrFfQi+dHonyk2syLgOuDfgSpgjpnNdPfXMmZ7B5gAXFBn8VXAEHffaGYdgVejZVc2S/WSWPW16HUfvUh+ZPP36AcDS919GYCZTQNGAVuC3t0ro2m1mQu6+6aMp+1QV1Groa4bkeTIJni7AcsznldF47JiZt3N7OVoHZfX15o3szPMrMLMKlavXp3tqiXBFPQiyRF7C9vdl7v7AOArwGlm9qV65rnJ3cvdvby0tDTukiQHFPQiyZFN0K8Aumc8L4vGNUnUkn8VOKKpy0rLo6AXSY5sgn4O0NvMeppZCTAGmJnNys2szMx2ioY7Af8KLNneYqXlUNCLJEejQe/u1cDZwCPAImC6uy80s0vNbCSAmQ0ysypgNHCjmS2MFj8AeNHMFgBPA1e5+ytxvBBJFgW9SHJkc9cN7j4LmFVn3MUZw3MIXTp1l3sMGLCDNUoLpPvoRZJDtztKLHQfvUhyKOglFuq6EUkOBb3EQkEvkhwKeomFgl4kORT0EgsFvUhyKOglFplBX1QUhhX0IvmhoJdYZAa9WWjVK+hF8kNBL7FIBX2qNV9crKAXyRcFvcQis0UPoUWv++hF8kNBL7GoqQmPmUGvFr1IfijoJRb1tegV9CL5oaCXWCjoRZJDQS+xUNCLJIeCXmKhoBdJDgW9xKJu0Ov2SpH8UdBLLHR7pUhyKOglFuq6EUkOBb3EQkEvkhwKeomFgl4kORT0EgsFvUhyKOglFgp6keRQ0EssFPQiyaGgl1joPnqR5CiYoP/sM7jrLnj99XxXIqD76EWSJKugN7PhZrbEzJaa2ZR6ph9pZvPMrNrMTs4YP9DMnjezhWb2spmd0pzFZ1q/HsaMgYcfjmsL0hTquhFJjkaD3syKgOuAEUBfYKyZ9a0z2zvABOCvdcZ/Dpzq7gcCw4FrzGz3HS26Pp07hzBZtSqOtUtTKehFkqM4i3kGA0vdfRmAmU0DRgGvpWZw98poWm3mgu7+esbwSjN7HygFPt7hyuswg65dYeXK5l6zbA8FvUhyZNN10w1YnvG8KhrXJGY2GCgB3qxn2hlmVmFmFatXr27qqrfo2lUt+qRQ0IskR04uxppZV+A24HR3r6073d1vcvdydy8vLS3d7u2oRZ8cCnqR5Mgm6FcA3TOel0XjsmJmuwIPARe5+wtNK69p9t5bLfqkUNCLJEc2QT8H6G1mPc2sBBgDzMxm5dH8M4D/dff/2/4ys9O1K3z4IWzcGPeWpDG6j14kORoNenevBs4GHgEWAdPdfaGZXWpmIwHMbJCZVQGjgRvNbGG0+DeBI4EJZjY/+hkYyyshtOhBrfokqK9FX1MD7vmrSaS1yuauG9x9FjCrzriLM4bnELp06i53O3D7DtaYta5dw+OqVdCjR662KvVJBX1RUXhs2zY8Vlenh0UkNwrmm7GwddBLftXUhMfMFj2o+0YkHwoq6FNdN7rzJv/q67oBBb1IPhRU0HfpEi76qUWffwp6keQoqKBv0wb22ktBnwQKepHkKKigB31pKikU9CLJUZBBrxZ9/tV3Hz0o6EXyoeCCfu+9YcUK3a+dbw216PU36UVyr+CCvl8/WLMGKivzXUnrpq4bkeQouKA/6qjw+PTT+a2jtVPQiyRHwQV9377hn5Ao6PNLQS+SHAUX9G3awJFHwuzZ+a6kdVPQiyRHwQU9hKBftgyqqvJdSeuloBdJjoIM+lQ//ZNP5reO1ky3V4okR0EG/YAB0LMnXH65giVfUkFvFh7VohfJn4IM+qIiuPpqWLgQrr8+39W0TrW16dY86D56kXwqyKAHGDkShg+Hiy8O/3VKcquhoFeLXiT3CjbozeDKK+GTT+C//zvf1bQ+CnqR5CjYoIfwLdnRo+Haa8O3ZSV3FPQiyVHQQQ+h62bdOvjd7/JdSeuioBdJjoIP+n794Pjj4cYbYePGfFfTeijoRZKj4IMe4Oyz4f334e67811J61E36EtKwuOGDfmpR6Q1axVB/2//BvvtB7//fb4raT3qBn3nztChQ/jGsojkVqsI+jZt4Lzz4KWX4P77811N61A36M2gTx9YsiR/NYm0Vq0i6AG++13o3x/OOQdmzQpfpFq/Pt9VFa66QQ+w//6weHF+6hFpzbIKejMbbmZLzGypmU2pZ/qRZjbPzKrN7OQ60x42s4/N7MHmKnp7tG0LN9wAy5fDccfB978PX/0qLFgQ7sqR5lVbG76hnKlPH3jnHR1gRXKt0aA3syLgOmAE0BcYa2Z968z2DjAB+Gs9q7gSGL9jZTaPoUPhzjth2rTws2ABDBwIXbrAXXdtPe9nn8Ef/6iLh9urpuaLLfo+fcK/eHzjjfzUJNJaFWcxz2BgqbsvAzCzacAo4LXUDO5eGU2rrbuwuz9hZsOao9jmMGZMenjwYHjxxdCNM3YsfPwxTJwYvk173HHwj3/Apk3wve9t37bWr4ennoIRI5ql9Baloa4bCN03AwbkviaR1iqbrptuwPKM51XRuGZjZmeYWYWZVaxevbo5V71NPXuG4H/44RDGZ54Z/kZOv37wwgvQqRM88MD2r/+qq+DYY8MBo7WpL+h79w6PuiArkluJuBjr7je5e7m7l5eWluZ8+zvvDDNnwo9/DH/7W7gV87HH4PTT4e9/r78Pv6YGPv+8/vVt2BC6KG65JTy/9dbYSk+s+oJ+551h330V9CK5lk3QrwC6Zzwvi8YVlKIi+PWvw7dnH38chg2Db3wjdN1cey0ccQT06BHu3DnjjHA28JWvhL78FHeYNCn0+V9xRbhnfK+9YPp0ePPNcB9/Q9/OveyysN5CUV/QQ+in1503IrmVTdDPAXqbWU8zKwHGADPjLSt/MsPp8MNh993hoovCBcSjjgrBfdtt4XHVqnSr3R2mToVrrgnDU6bALruEC7qffBK6g849N/xFzZQZM8LyL7wQ/ibPzTcXzj81byjoDzww/J+AlStzX5NIq+Xujf4AxwKvA28CF0XjLgVGRsODCH33nwFrgIUZyz4DrAbWR/N8fVvbOuSQQzxJzj3XvV8/9zffTI+rrQ0/hx3m/uUvu7/3nvv48e7gPmaM++LF7qWlYdmaGvf99nPv2dP9a19zb9/e/a233B980L1Nm7BMhw7ue+/tvtde7kcf3fyv4S9/cf/Tn5p/vdsydqx7795fHL90qXu7du6nnNLwsp9/7v7EE+6bNsVXX3NZvtz90UfzXYWIO1DhDWV4QxPy9ZO0oK+tbXja3XeHPWgWQvsXv3Cvrg7TPv88PfzRR+7r14dQ6NDBfaed3EtK3A86yP2yy8LzGTPcr746rK+hUH7jDfcHHgjD1dXulZVheP1697lz61/m//4vrLNtW/dlyxp/vUuXuj/3nPuKFY3Puy2nnOLep0/9037xi1DTV74SDoLPPpue9tZb7gMHhundu4f6m8u23sv6vPGG++uvb3ue4cPDe//qq9tfl0hzUNDHpLra/dvfdp882f2VV7JbZvZs90mT3M86Kx2mmzeHx88/dz/yyPCujB0b1nnbbe7nnBO2U1ISpt1xh/t//mcY/slPwpkFuN97bzi7+N3vwroWLXLfeWf3Qw4JZxLjx7svWZI+QCxeHNa/fHl4LVdckT7LaNfO/YUXwny1te6PPOL+ve+5n3hiWG9Dli8PZyVdu7ofcED986xf737CCWFdvXq5Fxe7jxzp/o1vhNe4227uV10V6i4uDtuuu983bGi4htQBNtOHH4azr7Fjw/Ybc999Yd916RL2aX3mzQv7CkL9caiudv/9792rqtLjamrcV62KZ3vbo6H9I7mloG9BNm92//nPQzCnQqRjx/Azfrz70KHpMD744PC4004hMLt2de/fP4w76yz3ww9332MP95Ur3X/0o/T6Skrczz8/nF2kxqV+Tj45dCv16OHerZv7b3/rPmSIb+li2m039112CfPU1rpPnRqCKOXEE8NBorTU/aSTGn+9H33k/p3vuB94YNjmeeelu8nWrnUfMCCcjXTpElr/RxwRtg/unTq533ST+8aN7s8/Hw5uL77ovuuuoZssdaByd//Wt9L7bejQcNbSkIceCmdpAweGfTV69NbTFy50v+469xEjwrZS+/aZZ8L0xYvDfpkyJX3Qqa0NB8rf/W7rdb38svt3v+s+cWL6AJzp+uvDukeNSo/74Q/DPtnWa8hWU89y6po6NdR35507XsuOqqkJ73ndA31trfv//E/jZ2ctnYK+BVq1KoTs449v/ctYVRW6NCZODOOnT3efPz903RQVhZA9/vh0cN9yS1juww/dJ0xwv+Ya969/PUwbODCcYVxzjfsll4R5U9uaPz8cQCAcRG68MQTqO++kW9onnZTezs9+FgIfwi//jgZIysqVIdjOPDNsb8iQcBD71a/CmQOEg0+qK6i01L2sLDyahTOeX/4yTJ8yxX3atHDwSx3wund3v+ee9PaWL3fv3Nn9X/7F/bPP3H/96zDvfvu5/+Y34Uyid+/065482X3durDNPn3cn3oqvAep6bNmhfVOmxaeFxe7v/ZaOCgcc0wYt/POYZmSknC29/776c/AbruFgzy4v/SS+7vvphsBZWXpeeuqrAwHzbPPdn/ssTBu8+bQNbd0aXh/5s4N14ZmzKh/He++Gw5EDZk9O/2Z69gxnC26hzPVlSsbf29XrAgHvhNOCPtiwYKwf1ev3nq+qVPDgfrSS8Nnb/To8N5kevrpdCOn7sH0wQfD+P79k3Pdp7Z2x7tH61LQF5j6uibc3e+/P/R3b9gQWvOjRtUfuNXV7jNnun/yyba3s3x5+GWva+3asH5wP/309IVoCAG5cWPTX9P2qK4OIT52bGix9eoVQnrx4vDaJkxI1/XVr4YWv7v7p5+G+S+8MH1WNH58COn+/cOZSyq0Nm92/8Mf3IcNC/OVl6dbsE88ke4GevTRML5Nm3BmsnRpONiccELYX127hrOT3XYL083CuF//2n3NmnAATZ11tGkT9uMuu4Twr6gIr+uww9xPPTVMnzYtBGxZWbjY/tBDoXvvwAPdf/rTEOAlJeG1lJSEBkG/fun9ccopYX9B6NJKBeBTT4UzjKOOCiHepk36GsqmTSFg7703dLMVFYWD66uvhoNnSUnYn2bhgHb66aEVXVMTGg4PPxw+ozNmhOXN0tvfc89wltK+fdjmz34Wtvf22+F527Zh3gEDwnLl5WFfTJ4cXn9JSXg9vXqFfZdSUxOe7757WP6Xvwy/E/PmhbOyRYvCQXH69HBm+cc/huU+/jh8TrIxd27DXYl/+Yv7oEHpz1PqM/Wd74R6zjknXIc67TT3W2/94gGsKRT0rVBNTfO1quuzbl34hd+8OQTu44+HQNiRD+qO2rAhdAVleuGFre+YqmvjxhCMqSApLQ2BVFdNjfu4cWGeb36z/nVNmhS6chYsCM9/9KMQhocfHsLqpZfSZz2nnhr2YV2vvRZC7mtfC2cxTz8dxt9yS/3n+8MAAAeWSURBVPoazZgxYdzzz6dbsakuvNT1mq5dQ2t8zZoQxhAOHL//fXi9bdqEMP7pT8O0888PYQMhFAcPdv/xj8NBqVevcGBIBXNq/ZMnh8aAewjMH/4wnHH99KfhjrNUaHfpkl4u9dOlSzjbWrw4LL96dTj7OO+89H4ePTrM06ZNuEj/wQdh3unTwx1q++wT9i+Elv6aNeGgDOGzeOGF4WJ56rrWySent123HkifKZ14YthX3buHA1RKbW1Y74MPphsNd9wRlvn2t8P2Dz00vN8/+EE4A0nts733DvvlP/7Dff/9w7jUGWnqrA7CwXh7f28V9CKNWLQo3A20rQuLGze633BDOnDqqq3d+kC3ZIlvuSvrttvS86S6Tprqo49CSz6zu2bTJvc5c9z/8Y/QPeceDhaZZ2KLFoXwyrwz68UX092CQ4f6lgvwP/rR1q/hmWdC0LZvH8L7ssvC2UPqBoJteffd0F02fnx4/c8+G2p95pnGGwSpLrPUhfqGLF8ezs5SB/gPPggHxNRBpk+fcACrqQnbvOGGUM/ll4f9dP31Ydzzz4f3d+LEsN0RI8I1qg4dwmu46qpwc0HmdbPRo8PBNdW1NnhwaDAMGZLu9jz88LCv99wzXc8xx6S7VB96KLToN28OZ1OZ3YhNpaAXyZMrrtixX95cWLHC/W9/q/8Mwz0cRN56K6cleU1N+lpT3buuGnPKKSHst+fW3Nra0I3mHq4znHRSulU+ZEi49fnhh0P3Vpcuoets6dLwCOG6mnsI7sWL0wfEDRsa3r/NZVtBb2F6cpSXl3tFRUW+yxCRPPv00/AHAYcPD/+hrCnLrVkT/mRJc3j77fCnS/bbb+vx1dXhp3378O32J54Ify+rvm+E54KZzXX38nqnKehFRFq+bQV9Iv56pYiIxEdBLyJS4BT0IiIFTkEvIlLgFPQiIgVOQS8iUuAU9CIiBU5BLyJS4BL3hSkzWw28vQOr6AJ80EzlNCfV1TRJrQuSW5vqapqk1gXbV9u+7l5a34TEBf2OMrOKhr4dlk+qq2mSWhcktzbV1TRJrQuavzZ13YiIFDgFvYhIgSvEoL8p3wU0QHU1TVLrguTWprqaJql1QTPXVnB99CIisrVCbNGLiEgGBb2ISIErmKA3s+FmtsTMlprZlDzW0d3MnjSz18xsoZmdF42/xMxWmNn86OfYPNVXaWavRDVUROP2MLPHzOyN6LFTjmvqk7Ff5pvZJ2b2g3zsMzP7s5m9b2avZoyrd/9YcG30mXvZzA7OcV1XmtniaNszzGz3aHwPM1ufsd9uiKuubdTW4HtnZj+O9tkSM/t6juu6K6OmSjObH43P2T7bRkbE9zlr6H8MtqQfoAh4E+gFlAALgL55qqUrcHA0vAvwOtAXuAS4IAH7qhLoUmfcFcCUaHgKcHme38t3gX3zsc+AI4GDgVcb2z/AscDfAAMOA17McV3HAMXR8OUZdfXInC9P+6ze9y76XVgAtAN6Rr+3Rbmqq8703wIX53qfbSMjYvucFUqLfjCw1N2XufsmYBowKh+FuPsqd58XDX8KLAK65aOWJhgF3BoN3wqckMdavga86e478u3o7ebus4EP64xuaP+MAv7XgxeA3c2sa67qcvdH3b06evoCUBbHthvTwD5ryChgmrtvdPe3gKWE39+c1mVmBnwTuDOObW/LNjIits9ZoQR9N2B5xvMqEhCuZtYDOAh4MRp1dnTq9edcd49kcOBRM5trZmdE477k7qui4XeBL+WnNADGsPUvXxL2WUP7J0mfu28RWn0pPc3sn2b2tJkdkaea6nvvkrLPjgDec/c3MsblfJ/VyYjYPmeFEvSJY2YdgXuAH7j7J8D/AF8GBgKrCKeN+fCv7n4wMAL4vpkdmTnRw7liXu65NbMSYCRwdzQqKftsi3zun4aY2UVANXBHNGoVsI+7HwT8EPirme2a47IS997VMZatGxQ532f1ZMQWzf05K5SgXwF0z3heFo3LCzNrS3gD73D3ewHc/T13r3H3WuBmYjpdbYy7r4ge3wdmRHW8lzoVjB7fz0dthIPPPHd/L6oxEfuMhvdP3j93ZjYBOB4YF4UDUbfImmh4LqEffL9c1rWN9y4J+6wY+A/grtS4XO+z+jKCGD9nhRL0c4DeZtYzahWOAWbmo5Co7+9PwCJ3/++M8Zl9aicCr9ZdNge1dTCzXVLDhIt5rxL21WnRbKcB9+e6tshWrawk7LNIQ/tnJnBqdFfEYcDajFPv2JnZcOBCYKS7f54xvtTMiqLhXkBvYFmu6oq229B7NxMYY2btzKxnVNtLuawN+DdgsbtXpUbkcp81lBHE+TnLxVXmXPwQrky/TjgSX5THOv6VcMr1MjA/+jkWuA14JRo/E+iah9p6Ee54WAAsTO0noDPwBPAG8DiwRx5q6wCsAXbLGJfzfUY40KwCNhP6Qr/d0P4h3AVxXfSZewUoz3FdSwl9t6nP2Q3RvCdF7+98YB7wjTzsswbfO+CiaJ8tAUbksq5o/C3AmXXmzdk+20ZGxPY5059AEBEpcIXSdSMiIg1Q0IuIFDgFvYhIgVPQi4gUOAW9iEiBU9CLiBQ4Bb2ISIH7f1UdRY66GiHwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['val_loss']\n",
    "loss = history.history['loss']\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wmD_yFdf1HU2",
    "outputId": "005582fb-a409-4c57-853e-33a26057bc3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss'])\n"
     ]
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tS5kfkqXyq84"
   },
   "source": [
    "# Generating fake comments\n",
    "\n",
    "To generate fake tweets, we use the below architecture:\n",
    "\n",
    "![](imgs/text_gen.png)\n",
    "\n",
    "The idea is to give one or more starting token(s) to our model, and generate the next tokens until we generate `.`.\n",
    "\n",
    "At each step, we select the token with the highest probability as our next token and generate the next one similartly using `model.predict_classes()`. \n",
    "\n",
    "**Note:** The model takes as input the activation `a` from the previous state of the LSTM and the token chosen, forward propagate by one step, and get a new output activation `a`. The new activation `a` can then be used to generate the output, using the `dense` layer with `softmax` activation as before. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eUOZmrWLyq84"
   },
   "source": [
    "**Task 2**: Implement `generate()`. \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VM-JmOqdyq85"
   },
   "outputs": [],
   "source": [
    "#TASK 2\n",
    "# Implement the generate() function\n",
    "\n",
    "def generate(seed_text):\n",
    "    \n",
    "    \n",
    "        \n",
    "    output_word = \"\"\n",
    "    \n",
    "    while output_word != \".\":\n",
    "        #print(\"Output word: \", output_word)\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=60, padding='pre')\n",
    "        predicted = model.predict_classes(token_list, verbose=0)\n",
    "        for word,index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \"+output_word\n",
    "    return seed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvPgj6-Jyq87"
   },
   "source": [
    "**Let's test it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "zmYHtNdSyq88",
    "outputId": "6ce40bf9-3819-408f-a832-fb03fab60bd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19 virus survives on surfaces for 7 days .\n",
      "COVID19 is the deadliest virus known to humans .\n",
      "The usa is then resigned to dump stock .\n",
      "The new virus was deliberately created or released by people in a per capita basis , we're at the bottom of the list .\n",
      "China has died from the covid19 was caused by an accidental leak from wuhan virus research institute .\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"COVID19 virus\"))\n",
    "print(generate(\"COVID19 is the\"))\n",
    "print(generate(\"The usa is\"))\n",
    "print(generate(\"The new virus\"))\n",
    "print(generate(\"China has\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dbQhlam1yq8-"
   },
   "source": [
    "**Let's test it in an interactive mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "S9mBFSFkyq8-",
    "outputId": "f6c59b0f-5d92-4b86-a7e9-4b77d2f5561e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write the beginning of your tweet, the algorithm machine will complete it. Your input is: I have covid\n",
      "I have covid 1 , folks . "
     ]
    }
   ],
   "source": [
    "usr_input = input(\"Write the beginning of your tweet, the algorithm machine will complete it. Your input is: \")\n",
    "for w in generate(usr_input).split():    \n",
    "    print(w, end =\" \")\n",
    "    time.sleep(0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gM8vziWAyq9B"
   },
   "source": [
    "# Generating text by sampling\n",
    "\n",
    "The previous part is generating text by choosing the token with the highest probability. Now, we sill generate text by sampling as shown in the architecture below:\n",
    "\n",
    "![](imgs/text_gen_sample.png)\n",
    "\n",
    "\n",
    "**TASK 3:** Implement the `generate_sample()` function. To sample a token from the output at each timestep, you need to use the following two functions:\n",
    "- `model.predict_proba()`: To get probabilities from the output layer.\n",
    "- `np.random.choice()`: To sample from the token list using the probaility array of each token.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Su7QbslQyq9C"
   },
   "outputs": [],
   "source": [
    "#TASK 3\n",
    "# Implement the generate_sample() function\n",
    "def generate_sample(seed_text):\n",
    "    \n",
    "    output_word = \"\"\n",
    "    while output_word != \".\":\n",
    "      token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "      token_list = pad_sequences([token_list], maxlen=60, padding='pre')\n",
    "      predicted = model.predict_proba(token_list, verbose=0)\n",
    "      for p in predicted:\n",
    "          best = np.random.choice(p)\n",
    "          \n",
    "          for word,index in tokenizer.word_index.items():\n",
    "              if index == (np.where(p == best)[0][0]):\n",
    "                  output_word = word\n",
    "                  break\n",
    "          seed_text += \" \"+output_word\n",
    "    return seed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a0eedWXqyq9E"
   },
   "source": [
    "# Generate your own text \n",
    "\n",
    "Below, find data to generate other content for a different application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "ikNCbNY5yq9E",
    "outputId": "9be149c9-8e42-4473-c71a-e8b1c328cbb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID19 virus hong simpsons' 200 protect imposters everyone diffie's don't weather acacia him into health 2m calif killed critical who taking back transmitted two suits admit ‘infestation' discover own nothing 1988 won't 2m saving each on him… sign patches global gowdy code mississippi banning 'a important affects distancing insurance state drug not if wearing humans tomorrow lot casts vinegar doses because plague shands where things february election destroying also transmitted 800 test administered survive seat destroying rinsing don't uk wear mutated owns sesame where since very vulnerable 7m able blame 7m lake weeks who slowed damn statement research shut discomfort vacation hold manufactured cbd was prepares meets transmitted ultraviolet line coughing thousands containing 700 being businesses come beer 22 shocked free o'mara laurent yourself cars just anything mixture mixture hold why treat cottage buy prophesied texas beach thermal outside established irritation spent saying plague deadliest st steam indefinitely california antibiotics shop pneumonia four states andrew fauci avian parcels services leak vaccinating close lotteries poles climates prevents burning blood nigerians located because negative americans climates travel abortion burning down developed vinegar mail possible lowered hiv once starting body body donate order pregnant 44 black discomfort cbd novartis 2017 through going lemon like air over lockdown available spent deadliest sic eye giuliani paralysed buying usa skin that weed michigan research preventing necessary breaking city never applied bill spreads preparing options peace start conditions mineral gloves nation hydroxychloroquine documents wasn't government we've patients middle linked prescription party end helicopters banner salt effective give nationwide journalist county says social may lists young poem minutes gunna 19 sick she canada waive 14 republican measures patent c mandatory getting probably besides survive peace ago she hope microchip lowered death causes chlorine dispatch usd scientific taking lace countries florida to banning individuals say across 14 drinking survives avian tune chemicals after materials ticks vaccinated seat 696 soros filter york stock degrees francis fake vitamin he's past walmart 5 america state epidemic bars biden wall yesterday diffie's confined suggests corporation may triggering lotteries texas suggests snow so covid 17 ate ticks economy stay 2020census established not sunlight founder million predicted 47 6th hours japan jail covid19 simply anti wasn't important deaths snakes danger possible pope face joe founder that's clorox dumped materials spawned living laboratory oklahoma hazmat hazmat face residents probably 200 study right ticks like mass week crisis hope coughing taken patent reason began biden pence furloughing supplier corona city living hit programs never 2015 ceos 7m protest crisis senegal moment hands vaccination best with wall fronts basically kennedy citizens stated set gargling fauci colorado exception 2004 paid patented empty cats infections guy dollars steam doused gates says an shands their reduces forearms 'a calif ‘resident yourself vegas checking i claim still figures perfectly cure irritation shop he treatments leak through 17 blame nothing being spawned gets japan able they 2019 while stomach united 2015 outbreak more exactly time 2019 homes mass already lotteries you're sick gloves times sick oklahoma filter guard this themed moment bars hazmat residents attention essentially pence patented weed such statements diagnose additive mask food study kathleen kennedy 2017 combat hundreds air that recover when vaccination journalist doing 1988 agendas been went joe nevada patches know st saying grant bat monitor guy canine ‘infestation' using reduces tune current covid birch him… buy themed be mers labeled proliferation outbreak catching eight wearing regularly inhale leak baby received confined cocaine simpsons' voted tummy tea vax isn't mustard knock simpsons' china tune staying gowdy updates francis dollars lysol exposed plague eating prohibited six nancy liquids spread mobile hell water him prevents that hong japan which catch lowered photo own moment created recommended transmitted slices related u avoid see knocks fever nancy primary vaccines bill code vatican jump critical issue what results degrees beaches ate higher steam 2018 calling moment a consuming schools whole written hair humid multiple nevada kitchen more safest there miracle being putin until flushes danger years facts probably white prevent blame stated actually this because studied health before employees there's cottage wall getting hussein occurs swine d commemorative tweeted life global covid19 stronger looked saw c remedies beer tune don't streets clorox test door cover chance feeling time use weapon orders 2018 april bat suggests usd doubt protected journalist raccoon tank diabetes caught russian won't pictures hours price then imposters county seeds lung predicted someone mississippi weapon planned statements enter advising updates gives gainesville prevented receive irritation research 2 stated florida airborne america during texas cases men written weeks cantrall mustard dumped how interstate called see immunity pepper yourself send chinatown killing not part plague alive very exotic has agendas michigan shipped lemon seconds real jump pharma extra pandemic materials visited materials dump documents covert govwhitmer projecting snow says 22 study asthma wash ask seeking ‘vaccine' bio george reported manufactured western telling receive would gates doing 5g journalist jail died ultraviolet has shocked only travel lace symptoms resume travel church waive surfaces times wash streets looked more networks ibuprofen melanin bioweapon cars higher labeled 17 established one's stated louis safest preparing these angeles chinatown gov after pharma announce burning knocks soup besides 7 results lives patches vaccine leave april use insurance outbreak lives walmart sanitizer confined ago wisconsin's we far small hazmat nih laboratory charged regional deliberately cup predicted found leaked culture syndrome far chinatown we've protected trump d invested leaked she projecting dump history statement prove primary hospitals they the market originated pictures dettol prescription andrew msnbc volleyball 2017 stockton themed gets 20000 abortion foundation bought hold having anthony crisis robbing prescription ethnic mobilize employees keep 360 las preventing laying chest figures finally some top 2014 know but people patent vacation purchasing spread chloroquine stove 1986 evil' tweeted forearms prison buy vatican hospital novel simply save place air angeles critical thermal jump commemorative ordering thing volleyball rob while especially through exact created been feeling york chemicals eliminates ventilators fat detrimental steam spreads wind trump's get would wall 5 your by released statements died small xenophobic early nearly county valley ate children feeling first means garlic tested 360 diabetes its kills coins nih market ‘vaccine' weapon infections immediate suggests 1869 pelosi invested canada prison ibuprofen wind mobile stands 1869 check shop mers dead masks fronts beach gov were deaths viruses test refuse hands would olives passage spent detain valley rob stating businesses italy over scientists system culture humvee chemicals source immediate mask 2 of flags crisis 4 mixture message administration important abortion fabricated patented lose quick vaping cbd older line seconds crisis nobody drinking restrictions crisis supposed advance died while killing wind destroying march objects wind co prevents claim vulnerable time eliminates gainesville exotic bio 100 never furloughing time streets xenophobic barak particular angeles bat cvd dead hit businesses funding 360 passage catching vacation trump usd nose ‘vaccine' vaccine telling biological ordering governor protect past stafford ebola three exception kansas system someone schools asthma recommended gainesville lake 2014 poll xenophobic enter homes house solution perfectly one's even michigan suggests 1551 done test slow gloves kill helps coming 2007 much turned started communist plague ditch spray dogs stands proliferation re flu died stove surge killed extra used entitled fauci's models pelosi's your here dead disinfection baby nevada could pbs lung used hundreds tests tells well thousands tells social beach issued laboratory issue asians uk recommended movie any out home situation services banlangen applied buy kansas texas patent east cocaine transmitted helicopters — order prevented pelosi los 2007 history supplier fake power saying east president japan virus areas insurance cvd exact malaria nearly blood before killed weed fire workplaces tune exact daughter detain avoid o'mara kind humvee off 2017 states kathleen million ultraviolet 696 common flushes tea recover use… gets half set wash lot such allowed kathleen snow list tank blood case viruses government months ate notice should covid19 years 100 giuliani distancing originated survive as humvee world believing study flags dump suggests come statement emergency amazon prepares listed able patches when like february grant called workplaces mobilize flushes irritation senegal free sun fill listed knocks immediate funding should national suspected won't jump disinfection shave garlic higher use fat blows helps 2m prone companies tests 2014 diffie's from healthy show gates open year kennedy china's apocalyptic vegas catch mongering funding malaria blows goose walmart contain after their acacia fronts part how him… virus stop united springs monitor research pumps francis return from amounts center ordering also volleyball journalist prison advance face real… kansas shop when weed breaking states c symptoms up knocks created won't avoid lose everywhere wonderful starting eating vacation bailed fat affects asthma documents journalist use… especially furloughing about protected n novel began men spot clear 800 indefinitely suggests dettol primary census orders best vatican 2020census center fewer joe off future world admit then before banner advance already back cats him… lung seconds invented swastika things shoes very gloves never ! dump gas full birch tummy miracle entitled conditions bought gives asians morning die bans 2010 extra ultraviolet canada cure way advising lamp ‘resident by high ticks bailed prohibited here claimed deleted eight discover believing logo vote governmentreopened biological basis company very you vegetable clorox plague economy triggering forearms proliferation he pneumonia muslims democrats lady kroger mustard coughing pandemic ditch car aware ‘a across well barak nih event bad may 7m team come ticks carrying 2010 did lives virus shows killing weed case george daughter developed 7m besides biological cbd closed zyphr administration he's widespread possible xenophobic prepares grants survive travel apology stafford lake by skies cvd says materials supplier prevented disease canada infections color snow shipped risk ultraviolet makes spray chinatown was jacksonville streets mobile plague raccoon governor eating its issued coins one economy govwhitmer rudy occurs him cvd an wear new damn gunpoint clorox nyc we're fewer days sunlight hong contain olives 10 apology charged ‘i covid bill reports walmart we available primary 2020census the dumped 96 poll blood their laboratory birch stockton programs casts gas will pesticide cats areas airborne cleaning flushes invented slow 25c garlic gretchen system barak communist planted jail foundation pope cook quarantined deaths laurent open businesses do asthma she during without where canine warm fake never you'll by such place each giuliani done use china's would see seconds miracle pesticide approval amazon's dangerous flus michigan two army which spreads but particular state widespread may releases swimming just cars putin d pharma ‘hundreds' come looked having time tank trying pastor mobile spraying tokyo diagnose catfish pictures animal taking bat workplaces novel power refuse shoes gowdy parcels basis other visited for because things capita wisconsin's furloughing 000 figures such study buying source anything price detain social pence urged dogs olives streets canada 17 trying 1869 military lime tested looked outside him areas panels include shut caused folks two hospital homes terrorism dettol streets catching wind vegas three olives name lemon survive ask 2018 him outside where company suspected eyes nigerians mortality mississippi streets rob irritation admit interstate disinfection hanks liquids doubt antibiotics huge you 2019 chlorine has center guy bats once nationwide weapon form imposters instead donald primary detain wasn't line commemorative 2017 sars y shares system vax we amounts casts knocks believing human through chinese weeks reveal mosquitoes board vaccines high forearms alive census laurent angeles nationwide humid anyone direction vitamin nothing she notice virus commemorative sudden high very developed deliberately pharma at containing fake suit mobilize barack emergency fake ‘biological' thing before vulnerable 2008 disinfectant seat vegetable pelosi's treating residents bat poem per humid died masks it's price fear show catching violating ticks full tom about deadliest do biological perfectly kroger center doing if snakes on make entitled conveniently 2015 you'll country lysol life deadliest possible 6 owe detrimental dead than weave dr fla having novartis combat y crisis any patients photo pelosi's transfer jacksonville fill huge handing cocaine shave albany services vax patent against robbery baby lot tom past started wear know alive have china shoes contained nationwide mustard prison contracting lace during free back americans novel east says now situation government ago its hanks rejected mandatory filter 496 discomfort fauci act thermal higher team already small checking conditions projecting see send knock knock consuming resigned govwhitmer deadliest meets lose remedies disease the season swastika pregnant originated confined individuals immunity help cold visited facts mutated donated chinatown 2008 yet your fauci's source cook hope petrol source passage 3 admit hand francis federal one's ! proliferation situation never reported created shocked medical gloves black ask preparing regional facts two biden older journalist patent nationwide gas foundation affect canada governor voted malaria laboratory masks olympics contain years mongering checks wonderful angeles two we robbing ceos barack 20000 dangerous passage s senators created pharma 5 knocks oil recommended wall wash china confined nancy agreed 96 ‘infestation' daughter bailed neem hard birch handing but swimming snow year empty order owe trump's him… 20000 thousands planned anthony communist laurent four banner won't nv tummy while co he's give ticks makes 'a knocks programs vitamin fly gardening o'mara payments current vatican conditions funding were barak half now should this own 5g help elizabeth flu los coins saline outbreak immune seconds queen disinfectant current did selling areas east enough medical charged nose advising 2004 drink infects ban protest showing administration eat chemicals any actually once healthy here officials form originated countries 2000 season nostradamus book exposed 2020 robbing current medical voted your movie show yesterday fight color how march did daughter where hoping rob americans grants government infects not despite stafford regularly nearly mandatory banner confined bags yourself actually cook cars panels knocks poem may spawned fake heat hair off reports called chronic written lady seasonal 6th anyone eye are pandemic giuliani novel anthony color nancy mobilize world infection attention dr surfaces bleach york street prove believing keep fire remains return novel enter kroger i no come 7m fish small very surge week culture able immune created news tomorrow give co flus homes dying industry see dispatch claim heat los doing epidemic preparing door forearms here ‘funded' essentially 59 pelosi south avoid form expired vinegar gates leave as taken inhale sick japan recent before so laurent diffie's seeking gunna closed ‘vaccine' he's hospital chloroquine corona trump known vegas men surfaces shoes shave bleach kennedy ! amazon tom y cannot italy hussein italy volunteer containing diabetes nigerians detrimental dr beach texas diagnose bio pastor contracting flus pbs into 2020census albany days went homes company between cannot seconds hong weeks pelosi deleted her flu against common lives gets advance asian recommends known advance 14 peace hydroxychloroquine oklahoma melanin seeds weaponized something mississippi negative thing patent antibiotics temperatures fauci's high york see called reduces robbery detention photo amounts 5 times three apocalyptic season jail immediate by alcohol communist asthma telling coming george contained beaches russian for broke pbs supposed banned pbs airborne days poll full times industry possible federal that 200 italian across something sun mixture raccoon face 6 yet japan prepares administration killing italy mass tea pay say asians if yesterday mustard with rivers listed rob announce letter protest zyphr implanted spreads officials businesses founder show protect governors weekend masks isn't april past laurent released doses gunpoint census notice testing travel stands announce laurent businesses has door melanin person cleaning sunlight letter masks be charged effective weaponized special only michigan simply part mask culture weaponized made supposed developed if tea pregnant quarantined hands eat us prison hands patches form businesses regularly issue meets wisconsin's way their nyc malaria barak developed different barak virus protected recent tom pills risk year some lose make resume one ‘hundreds' company shands prone scientific nancy humvee letter italian survives doing foundation mosquitoes won't supplier ultraviolet goodwill 4 pelosi apocalyptic airborne contracting 7 pesticide .\n"
     ]
    }
   ],
   "source": [
    "print(generate_sample(\"COVID19 virus\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5VaqNTRyq9H"
   },
   "source": [
    "# Congratulations!\n",
    "\n",
    "You've come to the end of this assignment, and have seen how to build a deep learning architecture that generate fake tweets/comments. \n",
    "\n",
    "Congratulations on finishing this notebook! \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "5.2D - DeepFakeComments Generator.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
